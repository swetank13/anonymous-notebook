{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731429c8-bae5-4455-921d-540c5d437dce",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Getting Started</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece378f4-7745-4945-bf4e-a10370ab16aa",
   "metadata": {},
   "source": [
    "- Deep learning is a machine learning technique that uses neural networks to learn from large amounts of data, mimicking the human brain's ability to recognize patterns and make decisions.\n",
    "- Why Deep Learning Needs More Data:\n",
    "  - Deep learning networks, especially deep neural nets and CNNs, have huge parameter counts and automatically learn high-level representations—this means more data is needed to avoid overfitting and leverage their modeling power.\n",
    "  - For small datasets, statistical models often outperform deep learning due to their built-in assumptions and feature engineering, which help with generalization when sample size is low.\n",
    "- Neural Network Architecture\n",
    "  - Feed Forward Neural Network (Something like a juicer :-P, Ex: Weather prediction, demand forecasting).\n",
    "  - Recurrent Neural Network (Something like a soup, prepared taste the output and again adjust salt, masal etc, Ex: Autonomous driving, photo classification, disease diagnosis).\n",
    "  - Convolutional Neural Network (Used in image or video use case, process each feature and faltten at the end, Ex: Machine translation, speech recognition (e.g., Google Assistant).\n",
    "  - Transformers (Ex: using tools like ChatGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7aaa81-1495-4e50-b292-d75c2621143f",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Neural Network: Fundamentals</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46413e4-db4d-40bb-81fc-f1d2cb458f1a",
   "metadata": {},
   "source": [
    "- Neurons\n",
    "  - Neurons are the basic building blocks of neural networks in deep learning.\n",
    "  - Neurons apply a weight to the input and use an activation function to determine the output.\n",
    "- The Purpose of Activation Function\n",
    "  - Real-world problems are often non-linear in nature, and activation functions help introduce non-linearity in a neural network.\n",
    "  - With activation functions, neurons either \"fire\" or \"do not fire.\" This can be thought of as individual detectives who are given a specific task and, after investigation, they give their findings to a judge (which is the neuron in the next layer).\n",
    "- In the case of the insurance prediction example, in the hidden layer, we have two detectives: one responsible for figuring out awareness and the other for affordability. They give their conclusion that a person's awareness is 0 or 1 (if using a step activation function), or it is a number between 0 to 1, say 0.7 (when using a sigmoid activation function).\n",
    "- Activation Functions: Sigmoid, ReLU, Tanh, SoftMax\n",
    "  - Sigmoid, Softmax, tanh, ReLU are the most commonly used activation functions.\n",
    "  - Sigmoid is primarily used in the output layer for binary classification problems (e.g., will a person buy insurance, is the transaction fraud).\n",
    "  - Softmax is primarily used in the output layer for multi-class classification problems (e.g., handwritten digits classification, clothes classification).\n",
    "  - tanh is similar to Sigmoid; the only difference is that the output range is -1 to 1 (for Sigmoid, the range is 0 to 1).\n",
    "  - ReLU is a default choice for neurons in hidden layers as it is fast to calculate and also doesn’t suffer much from vanishing gradient problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c0745-cbc3-4cf8-bd95-89c721f229d3",
   "metadata": {},
   "source": [
    "### <div align=\"center\">PyTorch</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002c284-cf81-4855-85e5-c203fc176dee",
   "metadata": {},
   "source": [
    "##### Matrix Fundamentals\n",
    "- In neural networks, weights can be efficiently multiplied with the output from the previous layer using matrix multiplication. If you are using a GPU, this becomes even faster as it will use multiple cores to compute dot products in parallel.\n",
    "- Neural networks require a lot of matrix multiplications and that is the reason why GPUs got very popular for deep learning as it helps in parallel processing.\n",
    "##### PyTorch Tensor Basics\n",
    "- Tensor is basic building blocks of deep learning.\n",
    "- - Tensor is a generic term for scaler\n",
    "  - O Dimension Tensor or Single variable or number called Scalar.\n",
    "  - 1 Dimension Tensor called Vector.\n",
    "  - 2 Dimension Tensor called Matrix.\n",
    "  - 3 Dimension Tensor called Cube.\n",
    "- Tensor can have any number of dimensions.\n",
    "- Using torch.Tensor, you can create a tensor object. Tensor objects look very much like numpy arrays. Numpy arrays can not be created on GPU directly whereas you can create a tensor object directly in GPU memory.\n",
    "- Tensor has numpy and dataframe like attributes such as dtype, shape, device etc.\n",
    "- view() method allows you to reshape the tensor.\n",
    "- zeros(), ones(), rand() can be used to create a new tensor with specific values.\n",
    "##### Autograd in PyTorch\n",
    "- Autograd feature allows to calculate gradients (i.e. partial derivatives) automatically. While training a neural network, we need to calculate gradients during backpropagation step. Automatic gradient calculation helps in this process.\n",
    "- torch.no_grad can be used if you want to temporarily stop calculating gradients.\n",
    "##### Numpy Arrays Vs PyTorch Tensors\n",
    "- PyTorch tensors and numpy arrays have similar functionality but tensor offers 3 key benefits over numpy arrays that are useful in deep learning.\n",
    "  - Benefit 1: Tensor come with in built support to leverage GPU acceleration.\n",
    "  - Benefit 2: Tensors have autograd features that computes gradients automatically. Numpy arrays do not have this feature.\n",
    "  - Benefit 3: Tensors are tightly integrated with PyTorch ecosystem that makes it easier to use with deep learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a10ff-d108-459d-8963-c7cca34487d1",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Neural Networks: Training</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c28a61-8d3d-4e3f-8b8b-92c0da0884df",
   "metadata": {},
   "source": [
    "##### Training though Backpropagation\n",
    "- Below are the 5 steps for Neural Network Training:\n",
    "  1. Initialize neural network with random weights\n",
    "  2. Feed forward training samples and calculate prediction error\n",
    "  3. Back propagate error to adjust weights using gradient descent\n",
    "  4. Repeat the process until certain number of iterations (epochs) or error is reduced significantly\n",
    "  5. Evaluate Neural Network Performance by using test or validation set\n",
    "- For training a neural network, we use a supervised training dataset. Feed samples one by one, calculate error and then backpropagate it to adjust weights.\n",
    "- The main objective of training is to find out the right weights for the neural network. It is like adjusting knobs on a sound board to get the expected audio output.\n",
    "- Error backpropagation uses partial derivatives to measure how much a specific weight contributes to an error. Based on that, adjustments are made to reduce the error in the next iteration.\n",
    "- One epoch is feeding all the records in your dataset through the network once during a training process.\n",
    "- MSE (Mean Squared Error) is one of the many cost functions used to measure error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9f314-4d2a-4583-a837-b9c008d760f1",
   "metadata": {},
   "source": [
    "##### Gradient Descent\n",
    "- Gradient Descent is a technique used in neural networks and statistical ML algorithms to find the optimal value of weights that results in minimal prediction error. That optimal point is also known as the global minimum.\n",
    "- It uses the gradient (or partial derivative) of error with respect to weights to perform weight adjustment.\n",
    "- `Learning rate` is a hyperparameter that we need to supply in gradient descent. It controls how much the weights adjust with each update\n",
    "##### Batch Gradient Descent Vs Mini Batch GD Vs Stochastic GD (SGD)\n",
    "- `Batch Gradient Descent` is an optimization algorithm that updates model parameters by calculating gradients across the entire dataset in each iteration to minimize a loss function.\n",
    "- `Mini Batch Gradient Descent` is a variant of batch gradient descent that updates model parameters by calculating gradients using a small subset of the dataset, balancing efficiency and stability.\n",
    "- `Stochastic Gradient Descent` updates model parameters using the gradient calculated from a single, random data point per iteration. This adds noise but speeds up convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2266fbc-7946-454b-bc7f-467dd5e98f93",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Neural Networks in PyTorch</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089445a-dc55-4b5f-a6f5-87e458d74481",
   "metadata": {},
   "source": [
    "- `nn.Module` is a base class for all neural network modules.\n",
    "- A usual practice is to create a subclass out of nn.Module to define your own neural network architecture.\n",
    "- Calling model(train_data) will internally call the forward method on your subclass.\n",
    "- Datasets and Data Loaders\n",
    "  - PyTorch provides a number of pre-loaded datasets that cover images, text, and audio.\n",
    "  - `DataLoader` lets you create batches from a large dataset easily for training. It also allows reshuffling the data at every epoch to reduce model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f590b-8a46-47b9-b55a-240299ce0ece",
   "metadata": {},
   "source": [
    "##### Cost Function - Binary Cross Entropy (a.k.a Log Loss)\n",
    "Reasons for Using Binary Cross Entropy\n",
    "- Aligns perfectly with probabilistic outputs, providing a natural fit for binary outcomes.\n",
    "- Produces convex cost function (when used with sigmoid) which is good for global minimum convergence.\n",
    "- Provides strong gradient updates, especially for confident, incorrect predictions.\n",
    "- Incorrect predictions are penalized logarithmically, encouraging accuracy and discouraging overconfidence in errors.\n",
    "- Cost functions like MSE may not work well for binary classification problems as the cost surface will not be convex and you may get stuck in local minima.\n",
    "- Binary Cross Entropy (BCE) along with sigmoid activation gives a smooth, convex surface for the cost function which makes convergence easier\n",
    "- BCE also penalizes high confidence errors, which eventually helps in efficient discovery of global minimum\n",
    "- BCE aligns perfectly with probabilistic outputs, providing a natural fit for binary outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64d9c2-282d-4bd7-9471-d4302e47e83a",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Model Optimization: Training Algorithm</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0ba1e-ff27-4824-aecf-89046bb16c3a",
   "metadata": {},
   "source": [
    "##### Model Optimization Overview\n",
    "- Model Optimization is a process of finding the best way to train a model such that we can train it faster by using less compute resources and the model performs well during prediction phase.\n",
    "- Model Optimization:\n",
    "  1. Training Algorithm\n",
    "  2. Regularization Techniques\n",
    "  3. Hyperparameter Tuning\n",
    "- Training Algorithm\n",
    "  1. Gradient Descent\n",
    "  2. GD with Momentum\n",
    "  3. RMSProp\n",
    "  4. Adam\n",
    "- Model Optimization is a process of finding the best way to train a model such that we can train it faster by using less compute resources and the model performs well during prediction phase.\n",
    "- Model optimization can be done using various ways such as Using different optimizers (GD, Momentum, RMSProp, etc.), Regularization techniques (L1, L2, Dropout), Hyperparameter tuning and so on.\n",
    "- `Exponentially Weighted Moving Average` (EWMA) gives more weight to recent data, smoothing out fluctuations over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28622eec-a031-46ee-aa30-31c2081b3e4f",
   "metadata": {},
   "source": [
    "##### Training Algorithm\n",
    "- Gradient Descent with Momentum\n",
    "  - GD with momentum accelerates convergence by building on past gradients, reducing the time to reach the minimum.\n",
    "  - The momentum term smooths out oscillations, especially in regions with steep, narrow valleys, leading to a more stable optimization path.\n",
    "  - Momentum helps GD escape small local minima and flat regions, making it more effective in complex loss landscapes.\n",
    "- RMSProp\n",
    "  - RMSProp uses an Exponentially Weighted Moving Average of squared gradients to reduce oscillations.\n",
    "  - Helps models converge faster and works well with noisy gradients.\n",
    "- Adam\n",
    "  - Adam combines momentum and RMSProp for efficient updates.\n",
    "  - Syntax: optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  - Tracks both mean and squared gradients to stabilize weight updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68910c-d148-4fa8-af7b-891cc7ae7d3d",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Model Optimization: Regularization Technique</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769761d-c617-46a1-911f-61555d38bc30",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "- Regularization is a set of techniques used to prevent overfitting — which is when a model performs well on training data but poorly on new, unseen data.\n",
    "- Regularization Techniques\n",
    "  1. Dropout Regularization: Dropout regularization drops certain neurons in each hidden layer during the training process. This generalizes the model and stops the network from learning specific details of training samples.\n",
    "  2. L1, L2 Regularization: Both L1 (Lasso Regression) and L2 (Ridge Regression) help prevent overfitting by adding a penalty to the cost function for large weights\n",
    "  3. Batch Normalization: Batch Normalization (BN) is a technique used in training deep neural networks to stabilize and accelerate the learning process. It also helps with regularization.\n",
    "     - Key Benefit\n",
    "       1. Stabilize learning\n",
    "       2. Higher Learning rate\n",
    "       3. Regularization effect\n",
    "     - Batch normalization normalizes layer inputs to have zero mean and unit variance, enhancing model performance.\n",
    "     - Allows for higher learning rates, reducing the training time for deep networks.\n",
    "     - Adds robustness to model initialization, making it less sensitive to initial weights.\n",
    "     - Reduces overfitting, particularly when used with dropout, by adding slight regularization effects.\n",
    "     - Learnable parameters gamma (scale) and beta (shift) allow the network to learn the optimal scale and mean for each feature.\n",
    "  4. Early Stopping\n",
    "     - Early Stopping monitors model performance on validation data to stop training when there is no further improvement for some fixed number of iterations.\n",
    "     - This parameter of fixed number of iterations with no improvement is called patience.\n",
    "     - It prevents overfitting by halting training before the model starts to memorize noise.\n",
    "     - Saves time and resources by avoiding unnecessary training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519d153-48c5-4016-a547-88aa29d1fdb8",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Model Optimization: Hyperparameter Tuning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00914300-af25-40ef-8f6a-248e65e07e3d",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "- Hyperparameter tuning is the process of systematically finding the best values for the hyperparameters of a machine learning model to optimize its performance.\n",
    "- Hyperparameter Tuning Benefits\n",
    "  1. Improve Model Accuracy\n",
    "  2. Reduce Overfitting, Underfitting\n",
    "  3. Optimize Training Time, Compute Resouces\n",
    "- Fine-tuning hyperparameters helps optimize model performance by selecting the best values for parameters like learning rate and batch size.\n",
    "- Unlike model parameters, hyperparameters are set before training and influence the learning process.\n",
    "- Effective tuning can prevent overfitting or underfitting, leading to better generalization on unseen data.\n",
    "- Hyperparameter tuning is crucial for enhancing the accuracy and efficiency of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50ec8b-8f83-4eb7-b4d8-1507d5fbd509",
   "metadata": {},
   "source": [
    "#### Optuna Hyperparameter Tuning\n",
    "- Optuna uses an efficient approach to find the best parameters using techniques such as:\n",
    "  1. Bayesian Optimization\n",
    "  2. Gradient Optimization\n",
    "  3. Evolutionary Algorithms\n",
    "- Optuna is a modern, automated hyperparameter optimization framework that uses an efficient, trial-based search.\n",
    "- It leverages techniques such as Bayesian optimization to find optimal hyperparameters faster than grid or random search.\n",
    "- Optuna allows for dynamic pruning, stopping unpromising trials early to save computation.\n",
    "- Ideal for deep learning tasks with large search spaces, where traditional tuning methods may be inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e599b42-4b96-4468-8dbd-acb3cbf20b8e",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Convolutional Neural Network (CNN)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c64ed-ae24-4fa0-b299-8f395a90af2f",
   "metadata": {},
   "source": [
    "- Disadvantages of FCN/FFN (Fully Connected/FeedForward Neural Networks) for image classification:\n",
    "  - High computation: Too many parameters and dense connections increase resource usage and training time.\n",
    "  - Loss of spatial data: Flattening destroys spatial relationships, so important patterns like edges/textures are ignored.\n",
    "  - Overfitting risk: Excess weights mean the model memorizes noise and outliers, harming generalization.\n",
    "- Filters or Kernels are nothing but the feature detectors.\n",
    "- `Pooling` reduces the size of feature maps (and thus computational requirements), decrease the risk of overfitting, and make the model more tolerant to variations and distortions in input data).\n",
    "  - Benefits of pooling\n",
    "    1. Dimension & Computation Reduction: Pooling downsamples spatial dimensions, resulting in smaller feature maps. This lowers the number of parameters and computations required in subsequent layers, making models faster and more memory-efficient.\n",
    "    2. Reduces overfitting: By summarizing features and discarding less important information, pooling provides regularization. This simplification helps prevent the network from memorizing the training data too closely.\n",
    "    3. Model is tolerant towards variantions, distortions etc: The model retains salient features, so its predictions do not change drastically if the input shifts or gets slightly altered.\n",
    "- CNN by itself doesn't take care of rotation and scale, we need to have rotated, scaled samples in training dataset. If we do not have such samples then use Data Augmentation to generate rotated, scaled images.\n",
    "- Convolutional Neural Networks (CNNs) excel at processing grid-like data such as images, identifying patterns through convolutions.\n",
    "- They use kernels (filters) to extract spatial features like edges and textures from input data.\n",
    "- CNN architectures combine convolution, pooling, and fully connected layers to learn hierarchical feature representations for tasks like image classification and object detection.\n",
    "- CNNs by design cannot handle scale and rotation. To address this Include images in the training dataset with variety in scale and rotation use data augmentation to generate new images from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68546e-fa17-45f7-8e1e-777efe96e94d",
   "metadata": {},
   "source": [
    "##### Padding and Strides\n",
    "- Padding preserves input dimensions during convolution, ensuring no loss of edge information.\n",
    "- Strides control the movement of the convolutional filter, affecting output size and computation speed.\n",
    "- Padding techniques like \"same\" and \"valid\" balance between maintaining dimensions and reducing output size.\n",
    "- Adjusting padding and strides can influence feature extraction granularity and network efficiency.\n",
    "##### Train a Neural Network\n",
    "- ReLU (Rectified Linear Unit): Removed all the black color from image and leave only white and due to this it break linearity. This is rough explanation.\n",
    "- In max pooling (Pooling and down sampling is samething) we keep the feature as it is by reducing the size and parameter significantly and avoiding overfitting.\n",
    "##### Data Augmentation\n",
    "- Data augmentation increases the diversity of the training dataset by applying transformations like rotation, flipping, and scaling.\n",
    "- Enhances model generalization by exposing it to varied scenarios, reducing overfitting.\n",
    "- Common techniques include geometric transformations, color adjustments, cropping, and adding noise.\n",
    "- Augmentation is performed dynamically during training, ensuring the model sees a new variation in each epoch.\n",
    "- Particularly effective in computer vision tasks where gathering more data can be costly or impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a90854-648b-470d-ad78-5190543ed877",
   "metadata": {},
   "source": [
    "##### Transfer Learning\n",
    "- Transfer learning is a machine learning technique where a pre-trained model on one task is reused or fine-tuned for a different but related task, such as using a model trained on cars to classify trucks.\n",
    "- Transfer learning leverages pre-trained models to solve new, related tasks with limited data.\n",
    "- It significantly reduces training time by reusing learned features from large datasets.\n",
    "- Commonly used in tasks like image classification and natural language processing to achieve high accuracy with minimal effort.\n",
    "- Transfer learning involves fine-tuning a pre-trained model or using it as a fixed feature extractor.\n",
    "- Ideal for scenarios with limited data, enabling effective learning without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de09ca-2200-4c7f-a85a-f21916fc8f2b",
   "metadata": {},
   "source": [
    "### <div align=\"center\">RNN (Recurrent Neural Networks) - Sequence Models</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bddf3-0920-4126-bbb3-2732985e5233",
   "metadata": {},
   "source": [
    "##### Sequence Model\n",
    "  - Sequential data refers to data where the order of elements matters, such as time series, text, audio, video, etc.\n",
    "  - Examples of sequence models include RNNs, LSTMs, GRUs, and Transformers, each designed for specific challenges.\n",
    "- Supervised\n",
    "  - Artificial Neural Network (ANN): Used for Regression and Classification\n",
    "  - Convolution Neural Network (CNN): Used for Computer Vision\n",
    "  - Recurrent Neural Network (RNN): Used for Time Series Analysis\n",
    "- Unsupervised\n",
    "  - Self-Organizing Maps: Used for feature detection\n",
    "  - Deep Boltzmann Machines: Used for Recommendation Systems\n",
    "  - Auto Encoder: Used for Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863cec1-4eb8-4f2a-a8aa-79383a93c932",
   "metadata": {},
   "source": [
    "##### Recurrent Neural Network (RNN)\n",
    "- Issues with regular neural network:\n",
    "  1. Regular neural networks require a fixed size input whereas sequences vary in length.\n",
    "  2. Regular neural networks do not consider order of elements in a sequence.\n",
    "  3. No parameter sharing\n",
    "- Benefits of RNN\n",
    "  - Designed to work with sequential data. Effective for tasks where order and context matter.\n",
    "  - In-built memory mechanism\n",
    "  - Parameter sharing\n",
    "- Recurrent Neural Networks (RNNs) are specialized for sequential data, processing inputs step-by-step while maintaining a memory of past information.\n",
    "- RNNs use hidden states to capture temporal dependencies, enabling predictions based on sequence history.\n",
    "- Ideal for tasks like text generation, speech recognition, and time-series forecasting.\n",
    "##### Types of RNN\n",
    "- One-to-Many RNNs generate sequences from a single input, like caption generation from an image.\n",
    "- Many-to-One RNNs summarize sequences into a single output, such as sentiment analysis of a sentence.\n",
    "- Many-to-Many RNNs handle sequence input and output, such as machine translation or video frame labelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c93cb6-4c0f-473c-aca3-92ba46382278",
   "metadata": {},
   "source": [
    "##### Vanishing Gradient Problem\n",
    "- The vanishing gradient problem in fully connected neural networks occurs when gradients shrink during backpropagation, preventing earlier layers from learning effectively.\n",
    "- Solutions for Vanishing Gradient\n",
    "  1. ReLU Activation\n",
    "  2. Batch Normalization\n",
    "  3. Residual Connections\n",
    "- The exploding gradient problem in fully connected neural networks occurs when gradients grow uncontrollably during backpropagation, causing unstable training and large weight updates.\n",
    "- Note: RNN learns via backpropagation through time.\n",
    "- Solutions to Vanishing Gradient Problem\n",
    "  1. LSTM\n",
    "  2. GRU\n",
    "  3. Residual Connections\n",
    "- Vanishing gradients occur when gradients become too small during backpropagation, hindering effective weight updates.\n",
    "- It primarily affects deep networks with activation functions like sigmoid or tanh, leading to slow or stalled learning.\n",
    "- Layers closer to the input experience smaller gradients, causing them to learn much slower than deeper layers.\n",
    "- Solutions include using activation functions like ReLU, batch normalization, or architectures like LSTMs with gating mechanisms.\n",
    "Addressing vanishing gradients is critical for training deep neural networks effectively and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57b711-09a3-4f44-9392-c1ab7a2259cd",
   "metadata": {},
   "source": [
    "##### LSTM (Long Short Term Memory Network)\n",
    "- Long Short Term Memory (LSTM) network addresses short term memory problem in RNN by introducing long term memory cell (a.k.a cell state).\n",
    "- It has both short term and long term memory.\n",
    "- It has 3 gates: Forget, Input, and Output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f66ada-be29-4a1e-aca2-614134f08b0c",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Transformers</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cafba5-b410-4e06-9de5-6a67b534383e",
   "metadata": {},
   "source": [
    "##### Overview of Encoder and Decoder\n",
    "- Transformer architecture has two parts:\n",
    "  1. Encoder\n",
    "  2. Decoder\n",
    "- The purpose of the encoder is to produce contextual embeddings for each word (more precisely, a token) in a given input sentence.\n",
    "- The purpose of the decoder is to produce an output sequence, which can be a word (for the next word prediction task) or a sequence (such as a translated sentence in case of language translation).\n",
    "  - 2 input will be the input to decoder context and output line (Take the highest probability word and start preparing the sentence).\n",
    "- Above points were related to inference stage not training (Normal flow Training -> Inference).\n",
    "- BERT and GPT are examples of specific models based on transformer architecture.\n",
    "- BERT has only encoder part where as GPT has only decoder part out of tranformer architecture.\n",
    "- BERT has 768 where as GPT has 12228 dimension.\n",
    "- Word embedding is a way to represent a word in a numeric format such that it captures the semantic meaning of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db236cf9-7433-4c3e-84ab-1a3880de3b42",
   "metadata": {},
   "source": [
    "##### Tokenization, Positional Embeddings\n",
    "- Inside the Encoder:\n",
    "  - Step 1: Generate Tokens and Token IDS.\n",
    "  - Step 2: Generate positional encoding from tokens (From Static Word Embeddings Matrix - When a model train it has Static Word Embeddings Matrix).\n",
    "- Tokens are similar to words (Ex: Sentence: i made sweet indian rice called kheer -> Token: [CLS] i made sweet indian rice call ed [SEP]. Each word has their index and based on that word will be converted to number).\n",
    "- BERT vocab size: 30522 and GPT3 vocab size: 50257.\n",
    "- Since models do the parallel processing hence along with Token, positional embedding vector will be their.\n",
    "##### Attention Mechanism\n",
    "- Attention mechanisms allow Transformers to focus on relevant parts of the input sequence for each output, improving context understanding.\n",
    "- Self-attention computes the relationships between all input elements, capturing dependencies regardless of their position.\n",
    "- Key components of attention include queries, keys, and values, which determine how much focus is given to different parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b9f12-a9ed-4698-980d-95d3149c08c2",
   "metadata": {},
   "source": [
    "##### Multi Headed Attention\n",
    "- Flow: I made sweet indian rice called Kheer -> Positional Embedding -> Attention Head (Wq - Query Vector, Wk - Key Vector, Wv - Value Vector) -> Context Aware Embedding.\n",
    "- The purpose of multiple attention heads is to allow the model to focus on different aspects or types of relationships between tokens (e.g., semantic, positional, syntactic) simultaneously, enriching the contextual understanding of each token.\n",
    "- The Feed-Forward Network (FFN) enriches each token's embedding by applying non-linear transformations independently, enabling the model to capture complex patterns and higher-order features beyond contextual relationships.\n",
    "- Normalization layer ensures stable learning improving the gradient flow.\n",
    "- Multi-headed attention enables Transformers to capture diverse relationships in the data by learning multiple attention patterns simultaneously.\n",
    "- Each attention head computes self-attention independently, focusing on different parts of the input sequence.\n",
    "- Outputs from all heads are concatenated and transformed to create a richer representation of the input.\n",
    "- Multi-headed attention improves the model's ability to understand complex patterns and long-range dependencies.\n",
    "- It is a key component in Transformer Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63055605-448e-4461-928a-58685fc868e5",
   "metadata": {},
   "source": [
    "##### Decoder\n",
    "- Output of encoder is contextual embedding.\n",
    "- The decoder in Transformer architecture generates the output sequence step-by-step, one token at a time.\n",
    "- It uses masked self-attention to ensure predictions depend only on previously generated tokens.\n",
    "- The decoder integrates encoder outputs through cross-attention to incorporate contextual information from the input sequence.\n",
    "- Fully connected layers in the decoder refine the processed information for final token prediction.\n",
    "- The decoder is central to tasks like language translation and text generation, where sequential output is crucial.\n",
    "- Below is the link to visually understand the Transformer Achitecture,\n",
    "  - https://poloclub.github.io/transformer-explainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb416e4b-9741-48ff-b99f-a4356c6114dc",
   "metadata": {},
   "source": [
    "##### How Transformers are trained ?\n",
    "- Based on previous word predicting the next word is called `Casual Language Modeling` (CLM). GPT is trained using CLM.\n",
    "- `Masked Language Modeling` (MLM): We take many words and mask some (ex: 15% is masked) tokens (words). It is bidirectional in nature. BERT (Google) is trained using MLM.\n",
    "- In Self-supervised learning, labels are generated from the data itself without requiring manual annotations.\n",
    "- Casual Language Modeling (CLM) and Mask Language Modeling (MLM) are self-supervised learning approaches used to train transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df4121-b4fe-45c8-8bfa-6a34c74303a8",
   "metadata": {},
   "source": [
    "- key feature of multi-headed attention in Transformers: It allows Transformers to capture multiple attention patterns simultaneously.\n",
    "- The attention mechanism in Transformers allow: To focus on relevant parts of the input sequence for each output.\n",
    "- Main purpose of BERT in NLP: The main purpose of BERT in NLP is to provide deep, bidirectional context-aware text representations, enabling models to understand the meaning of words based on their surrounding context.\n",
    "- Main purpose of word embeddings in NLP: To represent a word in a numeric format capturing its semantic meaning.\n",
    "- Main function of the decoder in Transformer models: To generate the output sequence step-by-step.\n",
    "- Popular technique for creating static word embeddings: Word2vec\n",
    "- Purpose of the encoder in Transformer architecture: To generate a contextual embedding for each word (token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173ef1f-3a01-4ed8-8306-1b565bd4b0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
