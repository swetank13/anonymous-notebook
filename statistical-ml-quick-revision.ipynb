{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76536ab-650f-4008-aada-8d91d4a5d31a",
   "metadata": {},
   "source": [
    "### <div align=\"center\">ML Basics</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce9521-6283-4c26-a676-d78e126eb9e1",
   "metadata": {},
   "source": [
    "#### Machine Learning is a discipline in computer science where we train machines on data so that they can make predictions without explicit programming. A machine is trained by providing the input and output data not the logic.\n",
    "Statistical ML setup the foundation of AI, Deep Learning is the natural extension of Statistical ML and together they form Machine Learning. ML gave birth to Neural Networks. In Neural Networks we see verity of architecture such as RNN, CNN, Transformers etc. Then NLP has come and combined with transformers and given birth to LLMs such as Gpt4.\n",
    "#### Two main categories under Machine Learning (ML)\n",
    "- Deep Learning: Under Deep Learning, it lists Neural Networks, CNN, and RNN.\n",
    "- Statistical ML: Under Statistical ML, it lists Linear Regression, Decision Tree, and K-means.\n",
    "- There is also a section on NLP Regular Expression and Robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b211767-daeb-4215-8330-bf2df183fa83",
   "metadata": {},
   "source": [
    "#### Classification Vs Regression\n",
    "- There are two major use cases of machine learning: Classification and Regression.\n",
    "- Classification is about dividing the data into specific groups, such as 'red' or 'blue', 'male' or 'female'.\n",
    "It is often used in fraud detection, news category classification etc.\n",
    "- Regression is about predicting a numerical value based on the previous values and its related features. it is often used to predict stock prices, housing prices, salaries etc.\n",
    "- Regression has continuous value where as classification has fixed category non continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a4e5c-7465-48b2-8f27-5d0c572da6ed",
   "metadata": {},
   "source": [
    "#### Supervised vs Unsupervised learning\n",
    "- Machine learning methods are broadly classified into three types\n",
    "  1. Supervised learning\n",
    "  2. Unsupervised learning.\n",
    "  3. Reinforcement learning\n",
    "- Supervised learning is a method in which the model is trained on a labeled dataset such as house price prediction.\n",
    "- Unsupervised learning is a method in which the input data is not provided with labels and the model is expected to classify the data based on the hidden patterns and structures like document classification.\n",
    "- Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and learns to maximize cumulative rewards over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bb60e-8efd-4a2d-a585-a41be832dd80",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Supervised ML Regression</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b05b66-6fba-4aa4-91d5-a69c71e4267c",
   "metadata": {},
   "source": [
    "- Simple Linear Regression\n",
    "  - Simple Linear Regression (One independent variable): y = mx + b (Slope (m) - coefficient, Intercept (b) - line cut at y axis, y is dependent variable).\n",
    "  - Linear regression helps to establish the relationship between dependent and independent variables.\n",
    "- Multiple Linear Regression: When we have more than one independent variable called Multiple Linear Regression.\n",
    "- Cost Function\n",
    "  - Mean Squared Error (MSE): The average of squared differences between prediction and actual observation. It effectively highlights larger errors.\n",
    "  - MSE = (1/n) Σ(yᵢ - ŷᵢ)²\n",
    "- Derivatives and Partial Derivatives\n",
    "  - Slope of a line at a given point is Derivative.\n",
    "  - Slope is used for linear equations, whereas Derivative is used for non-linear equations.\n",
    "  - Slope is constant, whereas Derivative is a function.\n",
    "  - The purpose of a Partial Derivative is to measure how a function changes as one of its variables is varied while keeping the other variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2993f206-f85d-4084-b740-b317f1c7794b",
   "metadata": {},
   "source": [
    "- Gradient Descent\n",
    "  - Gradient Descent: An optimization method used in linear regression to find the best-fit line by iteratively adjusting the slope (m) and intercept (b) to minimize the cost function, usually the mean squared error (MSE).\n",
    "  - Learning rate is step gradient descent take.\n",
    "  - Internally model.fit() uses gradient descent technic to train model and eventual goal is to come with coef and intercept.\n",
    "  - Adjusting the learning rate and epochs based on observed outputs (m, L) will enable you to obtain the desired outcomes in the Gradient Descent Implementation.\n",
    "- Why MSE (Not MAE)\n",
    "  - Mean Squared Error (MSE) is our go-to for calculating Gradient Descent because:\n",
    "    - 1. It's sensitive to outliers (Outlier is the data point far away from other data points, In this case error is higher).\n",
    "      2. It's continuously differentiable (MSE is a smooth and continuous function in terms of the parameters it depends on. This means its gradient (the derivative that tells us the direction to adjust the parameters) is also continuous and differentiable everywhere).\n",
    "- In rare scenarios, Mean Absolute Error (MAE) is our pick for Gradient Descent when we're dealing with lots of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea94cf-031e-415b-9455-21a2c04365b1",
   "metadata": {},
   "source": [
    "- Model Evaluation (Train, Test Split): We utilize the train_test_split() function from the sklearn library to split the dataset into training and testing parts and evaluating the model's precision. The parameter random_state is used in the train_test_split() function to ensure reproducibility.\n",
    "- Model Evaluation - Metrics\n",
    "  - To evaluate the performance of a Machine Learning model, we can use metrics such as MSE, MAE, or R2 score. The R2 score is easier to interpret, compared to other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304da12-f484-4798-8498-6cdb36ad0a0a",
   "metadata": {},
   "source": [
    "- Data preprocessing\n",
    "  - Data Science/AI Project Stages: Data Collection -> Data Preprocessing (Cleaning bad data, creating new features - feature engineering called one hot encoding) -> Model Training & Evaluation -> Model Tuning\n",
    "- Multicollinearity occurs when two or more independent variables are highly correlated, making it difficult to distinguish their individual effects on the dependent variable.\n",
    "- Overfitting and Underfitting:\n",
    "  - Overfitting: Occurs when a model learns too much detail and noise from the training data, affecting its performance on new data.\n",
    "  - Under fitting: Happens when a model is too simple and cannot learn the data pattern, leading to poor performance on all data.\n",
    "  - Balanced Fit: Achieved when a model accurately learns the training data's patterns and performs well on unseen data.\n",
    "- Reasons for Over fitting/Under fitting\n",
    "  1. Reason: Poor model, hyper parameters selection: Solution: Better model, hyper parameters selection (Where we try to cover each point and line becomes zigzag instead curve).\n",
    "  2. Reason: Insufficient training data, Solution: Sufficient training data\n",
    "  3. Reason: Poor feature selection, Solution: Careful feature selection\n",
    "  4. Reason: Inadequate validation, Solution: Adequate validation (If train and test datasets are from same area then points available in different area will not fit into derived model).\n",
    "  5. Reason: Lack of regularization, Solution: Apply regularization (It is a technique used to reduce overfitting but if we apply too much regularization then our model will go from one extream to another extream means overfit to underfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261e420-833b-42cf-a675-6fbaa98f73d8",
   "metadata": {},
   "source": [
    "- L1 and L2 Regularization\n",
    "  - L1 and L2 regularization are effective tools for minimizing over fitting. When L2 regularization is applied to Linear Regression, it transforms into Ridge Regression. In below use case we have added/increased the penalty and improved the regularization.\n",
    "  - In the same vein, L1 Regularization leads to what we commonly call Lasso Regression.\n",
    "  - The choice between Ridge or Lasso Regression and parameters like Alpha is contingent on multiple factors and is typically determined through a process of trial and error.\n",
    "- Bias Variance Trade off\n",
    "  - Bias is a measurement of how accurately a model can capture a pattern in a training dataset.\n",
    "  - Bias occurs when an algorithm misses significant patterns in the data due to its simplicity, while Variance occurs when an algorithm changes significantly based on minor differences in the training data.\n",
    "  - BUVO: Bias-Underfitting (High bias often leads to underfitting), Variance-Overfitting.\n",
    "  - Overfitting: Training Error - Low, Test Error - High (The model has learned too much detail from the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abca915-f56f-46a2-ae2c-26b8595d2abf",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Supervised Machine Learning - Classification</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059f473-e600-4c45-afed-2f93faf347b4",
   "metadata": {},
   "source": [
    "##### Introduction to Classification\n",
    "- Regression Models work with continuous values that can take on any value, while Classification is categorical and sticks to a definite set of values.\n",
    "- The Classification Model can be split into two types:\n",
    "  1. Binary Classification\n",
    "  2. Multi class Classification\n",
    "- Logistic Regression: Binary Classification:\n",
    "  - The Sigmoid Function converts input a range from 0 to 1.\n",
    "  - It is a crucial function in logistic regression for binary classification as it maps linear outputs to probabilities, helping to improve the model’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595bd16-b497-41ad-a730-833cde5dfd3b",
   "metadata": {},
   "source": [
    "##### Model Evaluation - Accuracy, Precision and Recall\n",
    "- We have two category for classification\n",
    "  - Truth (Actual Value)\n",
    "  - Prediction (Predicted Value)\n",
    "- `Accuracy` is the overall correctness of a model. It tells you the proportion of total predictions that were right. For example, if a model predicts whether emails are spam or not, accuracy is the percentage of all emails (spam and not spam) that the model classified correctly.\n",
    "- `Precision` is about correctness when the model predicts positive. It measures how many of the items predicted as positive are actually positive. For example, if the model says 10 emails are spam and 8 really are, precision is 8/10 = 80%. Precision cares about avoiding false alarms (false positives).\n",
    "- `Recall` (also called sensitivity) tells how well the model detects all the actual positive cases. It measures how many of the real positives the model successfully finds. For example, if there are 10 spam emails and the model correctly detects 9, recall is 9/10 = 90%. Recall cares about not missing positives.\n",
    "- Model Evaluation - F1 Score, Confusion Matrix\n",
    "  - The F1 score is the harmonic mean of the precision and recall.\n",
    "  - When we need balance between precision and recall then we use F1 score.\n",
    "  - - A `confusion matrix` is a table layout that visualizes the performance of a `classification` algorithm by displaying the true and false predictions it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba79d0c-38ff-446f-ad72-58446c555dfd",
   "metadata": {},
   "source": [
    "- Cost Function: Log Loss\n",
    "  - The Cost Function (MSE) will work for Linear Regressions as it is a convex function, but it won't work for Logistic Regression as it is a non-convex function.\n",
    "  - Log Loss is the underlying Cost Function that we use in Logistic Regression.\n",
    "  - Log Loss is also known as: Logistic Loss, Binary Cross Entropy, Multinomial Log Loss.\n",
    "- Support Vector Machine (SVM)\n",
    "  - Support Vector Machine (SVM) is a robust supervised learning model that finds the optimal hyperplane in an n-dimensional space for classification and regression tasks.\n",
    "  - A Kernel is a function that transforms data into a higher dimensional space so that a decision boundary can be drawn. There are different kernels available for different use cases, some examples - poly, linear, rbf, sigmoid.\n",
    "  - Gamma (γ): This is a parameter in SVM that decides the impact of each data point on the decision boundary. It is about how closely the model adheres to the data.\n",
    "  - Regularization: In machine learning, regularization adds a complexity penalty to the model to curb over fitting and enhance its performance on new data.\n",
    "  - Choosing the appropriate kernel significantly influences the total computational power. This, in turn, affects the budget of the entire project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8abefe-71c4-4785-8199-9de4ba47c111",
   "metadata": {},
   "source": [
    "- Scaling in machine learning involves adjusting the range of feature values to a common scale, such as 0 to 1, enhancing model performance by ensuring each feature contributes equally. Ex: Min-Max Scaling.\n",
    "##### Decision Tree Theory\n",
    "- In machine learning, a decision tree is a type of supervised learning algorithm. It models decisions and their potential outcomes in a structure resembling a tree, composed of various choices.\n",
    "- You can select the higher-level nodes using either of the following typical methods.\n",
    "  - Gini Impurity (Check formula)\n",
    "  - Entropy (Information Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea287fce-c476-4d51-b0fd-7c54f6ed00ea",
   "metadata": {},
   "source": [
    "#### Handle Class Imbalance\n",
    "- Class imbalance in machine learning occurs when the number of samples in each class is not equal, leading to a skewed distribution. There are several techniques to handle class imbalance in machine learning:\n",
    "  - Under Sampling Majority Class: Reduce instances in the majority class.\n",
    "  - Over Sampling minority class by duplication: Create more minority class instances by duplicating them.\n",
    "  - Over sampling minority class using SMOTE\n",
    "    - Generate synthetic examples using k nearest neighbors algorithm\n",
    "    - SMOTE – Synthetic Minority Over-sampling Technique\n",
    "  - Over sampling – SMOTE Tomek Links: Remove bridges (Tomek links) between neighboring minority and majority class instances\n",
    "  - Ensemble Method: Combine models to balance class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c96112-d55c-4e7c-865b-efaafadd3b96",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Ensemble Learning - Classification</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b93f1-efb4-4b66-a452-575cf884e866",
   "metadata": {},
   "source": [
    "#### What is Ensemble Learning ?\n",
    "- Ensemble learning is a strategy in machine learning where we join predictions from several models. This approach helps us get a more precise and resilient forecast. \n",
    "- It can be achieved using any of the techniques below\n",
    "  - Basic Techniques:\n",
    "    1. Majority Vote\n",
    "    2. Average\n",
    "    3. Weighted Average\n",
    "  - Advanced Techniques:\n",
    "    1. Bagging\n",
    "    2. Boosting\n",
    "    3. Stacking\n",
    "- The voting classifier (sklearn.ensemble.VotingClassifier) typically employs the \"hard\" method, which is based on majority rule voting. However, for classifiers that are well-calibrated, the \"soft\" method is suggested. This method determines class labels according to the argmax of the predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395fef1-6423-4dfe-9c81-c6640c3ea355",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Model Evaluation & Fine Tuning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0242ac-d019-4323-a09e-57db9c145b54",
   "metadata": {},
   "source": [
    "##### Model Evaluation - ROC (Receiver Operating Characteristic) Curve & AUC (Area Under Curve)\n",
    "- ROC curve: A graph showing the performance of a classification (Specially Binary Classification Model) model by plotting the true positive rate against the false positive rate at various threshold settings.\n",
    "- In Healthcare use cases we need high recall means if person has issue we want to capture that. In case spam email precision should be high, it means important email should not go to spam folder.\n",
    "- AUC: A metric that measures the entire two-dimensional area underneath the ROC curve, representing the model’s ability to distinguish between classes. When dealing with stakeholders with model metrics (ROC, AUC, precision, etc.), they won’t understand. The more AUC the better is model performance.\n",
    "##### K Fold Cross Validation\n",
    "- K-fold cross-validation is a method where the dataset is split into k subsets (or folds). The model is trained on k-1 folds and tested on the remaining fold. This process is repeated k times, ensuring each fold is used once as the test set, giving a reliable estimate of model performance.\n",
    "- Cross-validation can also be used with the same model but with different parameters.\n",
    "##### Stratified K Fold Cross Validation\n",
    "- Stratified K-Fold Cross Validation is a technique that divides data into K subsets while preserving the percentage of samples for each class, ensuring each fold is representative of the entire dataset.\n",
    "##### Hyperparameter Tuning: RandomizedSearchCV\n",
    "- The issue with GridSearchCV is that we need to try out all the combination which is quite expensive.\n",
    "- RandomizedSearchCV is useful when we have huge data and no of parameter want to try is huge.\n",
    "- RandomizedSearchCV is a technique that searches over random combinations of hyperparameters to find the best model configuration using cross-validation, making it more efficient than exhaustive methods.\n",
    "- To optimize computation, we primarily use RandomizedSearchCV. By adjusting just the n_iter parameter, it functions identically to GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8988165-16f2-4a72-90d7-6ebb76f2eb4e",
   "metadata": {},
   "source": [
    "#### Model Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ba5a98-f913-4fc8-9f25-6b94fba2ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"400\"\n",
       "            src=\"../../documents/model_selection_guide.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2762676e9e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='../../documents/model_selection_guide.pdf', width=1000, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d1a4a-d683-4989-992e-c2fcf92f3a6d",
   "metadata": {},
   "source": [
    "#### Selecting the Right Evaluation Metric\n",
    "- When we print the report we see 2 more parameter (apart from precision, recall etc.) macro avg (Useful in case of balance classes) and weighted avg (Useful in case of class imbalance).\n",
    "- Classifications:\n",
    "  1. Accuracy: Assesses the overall correctness of the model. Particularly useful when classes are balanced.\n",
    "  2. Precision: Indicates the reliability of positive predictions, especially important when false positives are costly.\n",
    "  3. Recall: Focuses on capturing as many positives as possible, crucial when missing a positive is costly.\n",
    "  4. AUC-ROC: Measures how well the model ranks positive instances higher than negative ones.\n",
    "  5. F1 Score: Balances precision and recall, providing a comprehensive performance metric.\n",
    "  6. Confusion Matrix: Offers insights into where the model is making errors, aiding in the assessment of accuracy, precision, recall, and other performance metrics.\n",
    "- Regressions:\n",
    "  1. MSE (Mean Squared Error): Highlights significant errors and is sensitive to outliers.\n",
    "  2. RMSE (Root Mean Square Error): Offers easier interpretation.\n",
    "  3. R² value: Indicates goodness of fit and measures the proportion of variance in the dependent variable predictable from the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489b361-6878-4def-9edb-39d2e9948dfa",
   "metadata": {},
   "source": [
    "#### Revise 7-ml-project-life-cycle\n",
    "### Feature Engineering\n",
    "##### 3 Ways of Doing Feature Engineering\n",
    "- Feature engineering can be done using two ways: business understanding and statistics.\n",
    "- 3 Ways of Doing Feature Engineering\n",
    "  1. Feature Cleaning & Transformation:\n",
    "     - Improving data quality by cleaning and transforming features to make them suitable for analysis\n",
    "     - Handle missing data\n",
    "     - Remove duplicates\n",
    "     - Treat outliers\n",
    "     - Scaling\n",
    "     - Normalization\n",
    "     - Encoding\n",
    "  3. Feature Selection: Choosing the most relevant features from the dataset to improve model performance and reduce complexity.\n",
    "  4. Feature Creation: Generating new features from existing data to enhance the predictive power of the model.\n",
    "##### Feature Selection using Corelation\n",
    "- Correlation in feature engineering feature selection: Measures the relationship between features to identify and remove redundant or irrelevant ones.\n",
    "- Scenarios where you should not use correlation for Feature Selection\n",
    "  1. Non-Linear Relationship\n",
    "  2. Outliers\n",
    "  3. Categorical variables\n",
    "  4. Correlation vs causation\n",
    "##### Feature Selection using Variance Inflation Factor (VIF)\n",
    "- Variance Inflation Factor (VIF) measures how much a feature's variance is increased due to multicollinearity, which is when features are highly correlated with each other.\n",
    "- High VIF means the feature is highly correlated with others, which can cause problems in the model. So we need to drop the column.\n",
    "- Multicollinearity may not affect the model’s accuracy directly, but model interpretation and the stability of coefficients will be negatively impacted.\n",
    "- Detect features with high Variance Inflation Factor (VIF) indicating multicollinearity → Eliminate the feature with the highest VIF → Recompute VIF for the remaining features → Repeat the process until all features have acceptable VIF values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35349e58-92be-4878-a47b-fb4776a14edf",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "##### K Means Clustering: Theory\n",
    "- K-means clustering is a method to group data into clusters where each piece of data is closest to the central point or centroid of its cluster.\n",
    "  1. Start with K centroids by putting them at random place. Here k = 2 (random).\n",
    "  2. Compute distance of every point from centroid and cluster them accordingly.\n",
    "  3. Adjust centroids so that they become center of gravity for given cluster.\n",
    "  4. Again re-cluster every point based on their distance with centroid.\n",
    "  5. Again adjust centroids.\n",
    "  6. Recompute clusters and repeat this till data points stop changing clusters.\n",
    "- SSE: Sum of Squared Errors\n",
    "  - To find the optimal number of clusters (k) using SSE, plot the sum of squared distances from each data point to its cluster's centroid. Then, select k where the decrease in SSE starts to level off, known as the elbow point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207b821-ba07-474f-8092-ec0f1c333ee3",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Data cleaning</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4273d22-1f05-4867-85ce-32add4bffb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find null value in dataframe for each column\n",
    "df.isna().sum()\n",
    "# Drop na value\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Distinct value and their count \n",
    "df['Category'].value_counts()\n",
    "\n",
    "# To analyze a column in a dataframe\n",
    "df.column_name.describe()\n",
    "\n",
    "# To fill the NA value with mean/median/mode\n",
    "df['column_name'].fillna(df['column_name'].mean(), inplace=True)\n",
    "\n",
    "# Replace space with underscore and convert to lower case\n",
    "df.columns = df.columns.str.replace(\" \",\"_\").str.lower()\n",
    "\n",
    "# Handle Duplicates\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Replace space with underscor and convert field to lower case\n",
    "df.columns = df.columns.str.replace(\" \",\"_\").str.lower()\n",
    "\n",
    "# Filter a column with condition and get unique value out of it\n",
    "df[df['number_of_dependants']<0]['number_of_dependants'].unique()\n",
    "\n",
    "# Get absolute value of a column in dataframe\n",
    "df['number_of_dependants'] = df['number_of_dependants'].abs()\n",
    "\n",
    "# Get columns which is of float64 or int64\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "# Get all the columns\n",
    "df.columns\n",
    "\n",
    "# Outlier Treatment: Income Column\n",
    "quantile_thresold = df1.income_lakhs.quantile(0.999) # Output: 100.0\n",
    "\n",
    "# Analyse Categorical Columns and get unique value for each column\n",
    "categorical_cols = ['gender', 'region', 'marital_status', 'bmi_category', 'smoking_status', 'employment_status', 'income_level', 'medical_history', 'insurance_plan']\n",
    "for col in categorical_cols:\n",
    "    print(col, \":\", df2[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80df11d-988e-437b-916b-f09ea2bde2d9",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Feature Engineering</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41e114-74d2-4ecb-915e-0e700f5989b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column by applying a transformation\n",
    "# Encode the categorical variable 'smoking_status' by converting 'Yes' to 1 and 'No' to 0.\n",
    "df['smoking_status'] = df['smoking_status'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "#Map the field value\n",
    "df['Gender'] = df['Gender'].map({'male': 1,  'female': 2})\n",
    "\n",
    "# add artificial genetical_risk column to the dataframe\n",
    "df['Genetical_Risk'] = 0\n",
    "\n",
    "# Define the risk scores for each condition\n",
    "risk_scores = {\"diabetes\": 6, \"heart disease\": 8, \"high blood pressure\":6, \"thyroid\": 5, \"no disease\": 0, \"none\":0}\n",
    "df2[['disease1', 'disease2']] = df2['medical_history'].str.split(\" & \", expand=True).apply(lambda x: x.str.lower())\n",
    "df2['disease1'] = df2['disease1'].fillna('none')\n",
    "df2['disease2'] = df2['disease2'].fillna('none')\n",
    "Diseases = ['disease1', 'disease2']\n",
    "df2['total_risk_score'] = 0\n",
    "for disease in Diseases:\n",
    "    df2['total_risk_score'] += df2[disease].map(risk_scores)\n",
    "\n",
    "# Encode Text Columns\n",
    "df2['insurance_plan'] = df2['insurance_plan'].map({'Gold': 3,'Silver': 2,'Bronze': 1})\n",
    "\n",
    "# Scale the column with min max scalar\n",
    "X = df4.drop('annual_premium_amount', axis='columns')\n",
    "y = df4['annual_premium_amount']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_scale = ['age','number_of_dependants', 'income_level',  'income_lakhs', 'insurance_plan', 'genetical_risk']\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "X.describe()\n",
    "\n",
    "# Calculate VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(data):\n",
    "    vif_df = pd.DataFrame()\n",
    "    vif_df['Column'] = data.columns\n",
    "    vif_df['VIF'] = [variance_inflation_factor(data.values,i) for i in range(data.shape[1])]\n",
    "    return vif_df\n",
    "\n",
    "calculate_vif(X)\n",
    "\n",
    "# we will drop income_lakhs due to high VIF value\n",
    "X_reduced = X.drop('income_level', axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42aec6a-103d-4cb0-a75a-0e856af634e8",
   "metadata": {},
   "source": [
    "- Decision tree (gini and entropy) is important because it is used in ensimble learning model like RandomForest xgboost etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8dbcc-4250-48e8-8bba-d1449a665e6e",
   "metadata": {},
   "source": [
    "<h3 align=\"center\" style=\"color:blue\">Model Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e6ea5-d163-4d26-8e4e-eb1135e29682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbb39f-6172-4b41-b761-aca5a569440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.30, random_state=10)\n",
    "\n",
    "# Linear Regression Model\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "test_score = model_lr.score(X_test, y_test)\n",
    "train_score = model_lr.score(X_train, y_train)\n",
    "train_score, test_score\n",
    "\n",
    "# Ridge Regression\n",
    "model_rg = Ridge(alpha=1)\n",
    "model_rg.fit(X_train, y_train)\n",
    "test_score = model_rg.score(X_test, y_test)\n",
    "train_score = model_rg.score(X_train, y_train)\n",
    "train_score, test_score\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_xgb = XGBRegressor(n_estimators=20, max_depth=3)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "model_xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d831277-da90-4b3d-abbd-d8aeb76d0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV\n",
    "model_xgb = XGBRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 40, 50],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "}\n",
    "random_search = RandomizedSearchCV(model_xgb, param_grid, n_iter=10, cv=3, scoring='r2', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "random_search.best_score_\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fd335-cea4-4099-92b4-56d35d658089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Model\n",
    "from joblib import dump\n",
    "\n",
    "dump(best_model, \"artifacts/model_rest.joblib\")\n",
    "scaler_with_cols = {\n",
    "    'scaler': scaler,\n",
    "    'cols_to_scale': cols_to_scale\n",
    "}\n",
    "dump(scaler_with_cols, \"artifacts/scaler_rest.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
